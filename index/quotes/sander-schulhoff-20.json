{
  "episode_id": "sander-schulhoff-20",
  "quotes": [
    {
      "quote_id": "sander-schulhoff-20_t1_q1",
      "topic_id": "sander-schulhoff-20_t1",
      "topic_title": "Introduction and Background",
      "text": "You can patch a bug, but you can't patch a brain. If you find some bug in your software and you go and patch it, you can be maybe 99.99% sure that bug is solved. Try to do that in your AI system. You can be 99.99% sure that the problem is still there.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:00:25",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=25s",
      "context": "A memorable mental model for why traditional security patching doesn\u2019t translate cleanly to AI systems and why vulnerabilities can remain persistent.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t1_q2",
      "topic_id": "sander-schulhoff-20_t1",
      "topic_title": "Introduction and Background",
      "text": "I found some major problems with the AI security industry. AI guardrails do not work. I'm going to say that one more time. Guardrails do not work. If someone is determined enough to trick GPT-5, they're going to deal with that guardrail. No problem. When these guardrail providers say, \"We catch everything,\" that's a complete lie.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:00:00",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=0s",
      "context": "A contrarian warning that \u201cguardrails\u201d should not be treated as comprehensive defenses, especially against motivated attackers.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t1_q3",
      "topic_id": "sander-schulhoff-20_t1",
      "topic_title": "Introduction and Background",
      "text": "The way he put it, the only reason there hasn't been a massive attack yet is how early the adoption is, not because it's secured.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:00:17",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=17s",
      "context": "A counterintuitive risk framing: lack of incidents so far may reflect low exposure rather than effective security.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t1_q4",
      "topic_id": "sander-schulhoff-20_t1",
      "topic_title": "Introduction and Background",
      "text": "And this has nothing to do with AGI. This is a problem of today, and the only reason we haven't seen massive hacks or serious damage from AI tools so far is because they haven't been given enough power yet, and they aren't that widely adopted yet.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:01:50",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=110s",
      "context": "An actionable prioritization cue for practitioners to treat prompt injection/jailbreak risk as a current production issue, not a future AGI concern.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t1_q5",
      "topic_id": "sander-schulhoff-20_t1",
      "topic_title": "Introduction and Background",
      "text": "Not only do you have a God in the box, but that God is angry, that God is malicious, that God wants to hurt you. Can we control that malicious AI and make it useful to us and make sure nothing bad happens?",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:00:43",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=43s",
      "context": "A vivid mental model for threat modeling: assume adversarial behavior and design controls accordingly rather than trusting default alignment.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t2_q1",
      "topic_id": "sander-schulhoff-20_t2",
      "topic_title": "Sponsor Break",
      "text": "Where other tools stop, Datadog goes even further. It helps you actually diagnose the impact of funnel drop-offs and bugs and UX friction.",
      "speaker": "Sponsor (Datadog)",
      "timestamp": "00:03:20",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=200s",
      "context": "Highlights an actionable product analytics approach: move from measuring metrics to diagnosing root causes and business impact.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t2_q2",
      "topic_id": "sander-schulhoff-20_t2",
      "topic_title": "Sponsor Break",
      "text": "Once you know where to focus, experiments prove what works.",
      "speaker": "Sponsor (Datadog)",
      "timestamp": "00:03:29",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=209s",
      "context": "A simple methodology for prioritization: identify the highest-impact area first, then validate with experimentation.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t2_q3",
      "topic_id": "sander-schulhoff-20_t2",
      "topic_title": "Sponsor Break",
      "text": "I saw this firsthand when I was at Airbnb where our experimentation platform was critical for analyzing what worked and where things went wrong.",
      "speaker": "Sponsor (Datadog)",
      "timestamp": "00:03:33",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=213s",
      "context": "Concrete example underscoring how experimentation infrastructure can be central to learning and debugging product changes.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t2_q4",
      "topic_id": "sander-schulhoff-20_t2",
      "topic_title": "Sponsor Break",
      "text": "And all of this is powered by feature flags that are tied to real-time data so that you can roll out safely, target precisely and learn continuously.",
      "speaker": "Sponsor (Datadog)",
      "timestamp": "00:03:56",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=236s",
      "context": "A practical operating model: combine feature flags with real-time data to enable safe rollouts, precise targeting, and continuous learning.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t2_q5",
      "topic_id": "sander-schulhoff-20_t2",
      "topic_title": "Sponsor Break",
      "text": "Metronome turns raw usage events into accurate invoices, gives customers bills they actually understand and keeps every team in sync in real time.",
      "speaker": "Sponsor (Metronome)",
      "timestamp": "00:04:44",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=284s",
      "context": "Actionable billing insight: treat billing as a real-time system that transforms usage telemetry into understandable invoices and cross-team alignment.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t3_q1",
      "topic_id": "sander-schulhoff-20_t3",
      "topic_title": "AI security scope and Sander\u2019s credentials",
      "text": "So basically we're going to be talking about AI security. And AI security is prompt injection and jailbreaking and indirect prompt injection and AI red teaming and some major problems I've found with the AI security industry that I think need to be talked more about.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:05:43",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=343s",
      "context": "Defines the practical scope of \u201cAI security\u201d as a concrete set of attack/defense areas practitioners can organize work around.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t3_q2",
      "topic_id": "sander-schulhoff-20_t3",
      "topic_title": "AI security scope and Sander\u2019s credentials",
      "text": "I ended up running the first ever generative AI red teaming competition. And I got a bunch of big companies involved. We had OpenAI, Scale Hugging Face, about 10 other AI companies sponsor it. And we ran this thing and it kind of blew up and it ended up collecting and open sourcing the first and largest data set of prompt injections.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:06:14",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=374s",
      "context": "Highlights a methodology\u2014competitive red teaming\u2014to generate real-world prompt-injection data at scale for testing and improvement.",
      "insight_type": "methodology"
    },
    {
      "quote_id": "sander-schulhoff-20_t3_q3",
      "topic_id": "sander-schulhoff-20_t3",
      "topic_title": "AI security scope and Sander\u2019s credentials",
      "text": "That paper went on to win the best theme paper at EMNLP 2023 out of about 20,000 submissions. And that's one of the top natural language processing conferences in the world. The paper and the dataset are now used by every single Frontier Lab and most Fortune 500 companies to benchmark their models and improve their AI security.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:06:14",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=374s",
      "context": "Provides a concrete adoption/credibility data point: the dataset is widely used for benchmarking and improving AI security.",
      "insight_type": "data"
    },
    {
      "quote_id": "sander-schulhoff-20_t3_q4",
      "topic_id": "sander-schulhoff-20_t3",
      "topic_title": "AI security scope and Sander\u2019s credentials",
      "text": "And AI guardrails are one of the more common defenses. And it's basically, for the most part, it's a large language model that is trained or prompted to look at inputs and outputs to an AI system and determine whether they are valid or malicious or whatever they are.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:07:34",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=454s",
      "context": "Gives a clear mental model of what \u201cguardrails\u201d typically are in practice\u2014an LLM-based classifier over inputs/outputs.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t3_q5",
      "topic_id": "sander-schulhoff-20_t3",
      "topic_title": "AI security scope and Sander\u2019s credentials",
      "text": "And what I have found through running these events is that they are terribly, terribly insecure and frankly, they don't work. They just don't work.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:07:34",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=454s",
      "context": "A counterintuitive claim from repeated red-teaming: common LLM guardrail defenses fail in practice, implying teams need stronger approaches than \u201cLLM watching LLM.\u201d",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t4_q1",
      "topic_id": "sander-schulhoff-20_t4",
      "topic_title": "Jailbreaking vs prompt injection explained",
      "text": "So the difference is in jailbreaking, it's just a malicious user and a model. In prompt injection, it's a malicious user, a model, and some developer prompt that the malicious user is trying to get the model to ignore.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:08:38",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=518s",
      "context": "A crisp mental model practitioners can use to classify threats and decide whether to focus on model behavior or instruction-boundary enforcement.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t4_q2",
      "topic_id": "sander-schulhoff-20_t4",
      "topic_title": "Jailbreaking vs prompt injection explained",
      "text": "So jailbreaking, no system prompt. Prompt injection, system prompt, basically. But then there's a lot of gray areas.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:09:39",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=579s",
      "context": "A practical rule-of-thumb for threat modeling, with an explicit warning that real systems blur the boundary.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t4_q3",
      "topic_id": "sander-schulhoff-20_t4",
      "topic_title": "Jailbreaking vs prompt injection explained",
      "text": "I discovered a combination of behaviors within ServiceNow Assist AI implementation that can facilitate a unique kind of second order prompt injection attack. Through this behavior, I instructed a seemingly benign agent to recruit more powerful agents in fulfilling a malicious and unintended attack, including performing create, read, update, and delete actions on the database and sending external emails with information from the database.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:09:54",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=594s",
      "context": "A concrete multi-agent prompt-injection example showing how an attacker can escalate privileges via agent-to-agent delegation to trigger real-world actions.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t4_q4",
      "topic_id": "sander-schulhoff-20_t4",
      "topic_title": "Jailbreaking vs prompt injection explained",
      "text": "It's really important for people to understand that none of the problems have any meaningful mitigation. The hope the model just does a good enough job and not being tricked is fundamentally insufficient. And the only reason there hasn't been a massive attack yet is how early the adoption is, not because it's secured.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:11:23",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=683s",
      "context": "A contrarian warning that \u201cmodel robustness\u201d isn\u2019t a security strategy, and the lack of major incidents is likely due to low adoption rather than strong defenses.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t5_q1",
      "topic_id": "sander-schulhoff-20_t5",
      "topic_title": "Real-world attack examples and patterns",
      "text": "Somebody realized that if you get it to write malicious code, you can exfiltrate application secrets and kind of do whatever to that app.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:12:52",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=772s",
      "context": "Highlights the concrete risk pattern of executing LLM-generated code on the same server as sensitive secrets.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t5_q2",
      "topic_id": "sander-schulhoff-20_t5",
      "topic_title": "Real-world attack examples and patterns",
      "text": "And so both of those examples are prompt injection where the system is supposed to do one thing.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:14:02",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=842s",
      "context": "Provides a crisp mental model for classifying prompt injection: steering a system away from its intended task.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t5_q3",
      "topic_id": "sander-schulhoff-20_t5",
      "topic_title": "Real-world attack examples and patterns",
      "text": "Instead of having a regular computer virus, you have a virus that is built on top of an AI and it gets into a system and it kind of thinks for itself and sends out API requests to figure out what to do next.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:15:57",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=957s",
      "context": "Describes an emerging attacker methodology: AI-driven malware that adapts via iterative API calls.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t5_q4",
      "topic_id": "sander-schulhoff-20_t5",
      "topic_title": "Real-world attack examples and patterns",
      "text": "but also if you separate your requests in an appropriate way, you can get around defenses very well.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:15:57",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=957s",
      "context": "Actionable red-team insight: many safeguards fail when harmful intent is decomposed into benign-looking steps.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t5_q5",
      "topic_id": "sander-schulhoff-20_t5",
      "topic_title": "Real-world attack examples and patterns",
      "text": "So a lot of the way they got around these defenses was by just kind of separating their requests into smaller requests that seem legitimate on their own, but when put together are not legitimate.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:17:19",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1039s",
      "context": "Gives a practical attack pattern practitioners can test against: multi-step \u201cbenign\u201d prompts that compose into an unsafe outcome.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t6_q1",
      "topic_id": "sander-schulhoff-20_t6",
      "topic_title": "Why agents and robots raise the stakes",
      "text": "But with agents, there's all types of bad stuff that can happen. And if you deploy improperly secured, improperly data-permissioned agents, people can trick those things into doing whatever, which might leak your user's data and might cost your company or your user's money, all sorts of real world damages there.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:18:27",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1107s",
      "context": "Highlights a practitioner-relevant risk model: once agents can act, security and data permissions become the primary failure modes with direct real-world impact.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t6_q2",
      "topic_id": "sander-schulhoff-20_t6",
      "topic_title": "Why agents and robots raise the stakes",
      "text": "with chatbots, as you said, very limited damage outcomes that could occur, assuming they don't invent a new bioweapon or something like that.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:18:27",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1107s",
      "context": "A counterintuitive framing that today\u2019s chatbot harms are comparatively bounded\u2014implying the risk profile changes dramatically with autonomy.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t6_q3",
      "topic_id": "sander-schulhoff-20_t6",
      "topic_title": "Why agents and robots raise the stakes",
      "text": "And we're going into robotics too, where they're deploying VLM, visual language model, powered robots into the world and these things can get prompt injected.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:19:11",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1151s",
      "context": "Extends prompt-injection from software to embodied systems, signaling a new attack surface practitioners must plan for.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t6_q4",
      "topic_id": "sander-schulhoff-20_t6",
      "topic_title": "Why agents and robots raise the stakes",
      "text": "if you're walking down the street next to some robot, you don't want somebody else to say something to it that tricks it into punching you in the face, but that can happen.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:19:11",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1151s",
      "context": "A vivid example of how prompt injection can translate into immediate physical harm when models control robots.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t6_q5",
      "topic_id": "sander-schulhoff-20_t6",
      "topic_title": "Why agents and robots raise the stakes",
      "text": "We've already seen people jailbreaking LM powered robotic systems, so that's going to be another big problem.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:19:11",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1151s",
      "context": "Points to an observed, real-world precedent (jailbreaking robots), underscoring that this is not a hypothetical threat.",
      "insight_type": "data"
    },
    {
      "quote_id": "sander-schulhoff-20_t7_q1",
      "topic_id": "sander-schulhoff-20_t7",
      "topic_title": "AI security industry landscape and offerings",
      "text": "And I'll quickly differentiate and separate out the Frontier Labs from the AI security industry because there's the Frontier Labs and some Frontier adjacent companies that are largely focused on research like pretty hardcore AI research. And then there are enterprises, B2B sellers of AI security software.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:20:12",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1212s",
      "context": "Provides a useful mental model for separating frontier research efforts from the enterprise vendor ecosystem when evaluating \u201cAI security.\u201d",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t7_q2",
      "topic_id": "sander-schulhoff-20_t7",
      "topic_title": "AI security industry landscape and offerings",
      "text": "And if you look at the market map for this, you see a lot of monitoring and observability tooling. You see a lot of compliance and governance, and I think that stuff is super useful. And then you see a lot of automated AI red teaming and AI guardrails. And I don't feel that these things are quite as useful.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:20:48",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1248s",
      "context": "A contrarian take that prioritizes monitoring/compliance over guardrails/red-teaming tools, challenging common buying instincts.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t7_q3",
      "topic_id": "sander-schulhoff-20_t7",
      "topic_title": "AI security industry landscape and offerings",
      "text": "So the first aspect, automated red teaming are basically tools, which are usually large language models that are used to attack other large language models.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:21:18",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1278s",
      "context": "A crisp definition practitioners can use to evaluate what \u201cautomated red teaming\u201d products actually do.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t7_q4",
      "topic_id": "sander-schulhoff-20_t7",
      "topic_title": "AI security industry landscape and offerings",
      "text": "And then there are AI guardrails, which as we mentioned, are AI or LLMs that attempt to classify whether inputs and outputs are valid or not.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:22:10",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1330s",
      "context": "Defines guardrails in operational terms (classification of inputs/outputs), clarifying what they can and can\u2019t reliably accomplish.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t7_q5",
      "topic_id": "sander-schulhoff-20_t7",
      "topic_title": "AI security industry landscape and offerings",
      "text": "So one guardrail watches all inputs, and if it sees something like, \"Tell me how to build a bomb,\" it flags that. It's like, \"Nope, don't respond to that at all.\" But sometimes things get through. So you put another guardrail on the other side to watch the outputs from the model, and before you show outputs to the user, you check if they're malicious or not.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:22:10",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1330s",
      "context": "Gives a concrete deployment pattern (input + output guardrails) and highlights the practical reality that filters can miss things.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t8_q1",
      "topic_id": "sander-schulhoff-20_t8",
      "topic_title": "Adversarial robustness and how it\u2019s measured",
      "text": "This ServiceNow example, actually, interestingly, ServiceNow has a prompt injection protection feature and it was enabled as this person was trying to hack it and they got through.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:23:29",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1409s",
      "context": "Concrete example showing that even enabled guardrails can fail under real attacks, motivating robust measurement.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t8_q2",
      "topic_id": "sander-schulhoff-20_t8",
      "topic_title": "Adversarial robustness and how it\u2019s measured",
      "text": "Adversarial robustness. Yeah. So this refers to how well models or systems can defend themselves against attacks.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:23:57",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1437s",
      "context": "Defines the core framework practitioners use to reason about and evaluate AI system defensibility.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t8_q3",
      "topic_id": "sander-schulhoff-20_t8",
      "topic_title": "Adversarial robustness and how it\u2019s measured",
      "text": "But if you have one of those like guardrail, then LLM, then another guardrail system, you can also use it to describe the defensibility of that term.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:24:00",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1440s",
      "context": "Extends robustness thinking from just the model to the full stacked system architecture (guardrails + LLM + guardrails).",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t8_q4",
      "topic_id": "sander-schulhoff-20_t8",
      "topic_title": "Adversarial robustness and how it\u2019s measured",
      "text": "You'd never actually say this in practice because it's very difficult to estimate adversarial robustness because the search space here is massive, which we'll talk about soon.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:24:00",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1440s",
      "context": "Counterintuitive caution that robustness percentages are hard to justify due to the enormous attack space.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t8_q5",
      "topic_id": "sander-schulhoff-20_t8",
      "topic_title": "Adversarial robustness and how it\u2019s measured",
      "text": "So ASR is the term you'll commonly hear used here, and it's a measure of adversarial robustness. So it stands for attack success rate.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:25:01",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1501s",
      "context": "Names the key metric vendors use to quantify robustness, which practitioners should understand and scrutinize.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t9_q1",
      "topic_id": "sander-schulhoff-20_t9",
      "topic_title": "How enterprises buy guardrails and audits",
      "text": "And they come in and they do kind of a security audit and they go and they apply their automated red teaming systems to the models I'm deploying.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:25:58",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1558s",
      "context": "Describes a common enterprise implementation pattern: audits plus automated red-teaming as the entry point for AI security vendors.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t9_q2",
      "topic_id": "sander-schulhoff-20_t9",
      "topic_title": "How enterprises buy guardrails and audits",
      "text": "And they find, oh, they can get them to output hate speech, they can get them to output disinformation CBRN, all sorts of horrible stuff.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:25:58",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1558s",
      "context": "Gives concrete examples of the failure modes that show up in red-team results and drive urgency in procurement decisions.",
      "insight_type": "data"
    },
    {
      "quote_id": "sander-schulhoff-20_t9_q3",
      "topic_id": "sander-schulhoff-20_t9",
      "topic_title": "How enterprises buy guardrails and audits",
      "text": "And the guardrails company is like, \"Hey, no worries. We got you. We got these guardrails.\"",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:25:58",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1558s",
      "context": "Captures the sales/solution framing that often follows alarming audit findings\u2014positioning guardrails as the immediate fix.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t9_q4",
      "topic_id": "sander-schulhoff-20_t9",
      "topic_title": "How enterprises buy guardrails and audits",
      "text": "And I go and I buy their guardrails and their guardrails kind of sit in front of and behind my model and watch inputs and flag and reject anything that seems malicious and great.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:25:58",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1558s",
      "context": "Provides an actionable architectural mental model for how guardrails are typically deployed (pre- and post-model filtering).",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t10_q1",
      "topic_id": "sander-schulhoff-20_t10",
      "topic_title": "Why automated red teaming overstates novelty",
      "text": "The first one is those automated red teaming systems are always going to find something against any model.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:28:33",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1713s",
      "context": "Sets the expectation that automated red teaming outputs will reliably surface issues, so practitioners should interpret findings as baseline rather than surprising breakthroughs.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t10_q2",
      "topic_id": "sander-schulhoff-20_t10",
      "topic_title": "Why automated red teaming overstates novelty",
      "text": "And because all, I guess for the most part, all currently deployed chatbots are based on transformers or transformer adjacent technologies, they're all vulnerable to prompt injection gel breaking forms of adversarial attacks.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:28:33",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1713s",
      "context": "Provides a mental model that the vulnerability is architectural and broadly shared across deployed LLMs, informing threat modeling and risk communication.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t10_q3",
      "topic_id": "sander-schulhoff-20_t10",
      "topic_title": "Why automated red teaming overstates novelty",
      "text": "And so, these automated red teaming systems are not showing anything novel.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:28:33",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1713s",
      "context": "Warns practitioners not to over-index on automated red team results as \u201cnew\u201d discoveries, especially when testing the same frontier models used in production.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t10_q4",
      "topic_id": "sander-schulhoff-20_t10",
      "topic_title": "Why automated red teaming overstates novelty",
      "text": "So the first problem is AI red teaming works too well.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:29:48",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1788s",
      "context": "Counterintuitively reframes \u201csuccessful\u201d red teaming as a problem because it can generate alarm without improving understanding of real-world risk.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t10_q5",
      "topic_id": "sander-schulhoff-20_t10",
      "topic_title": "Why automated red teaming overstates novelty",
      "text": "And that is AI guardrails do not work. I'm going to say that one more time. Guardrails do not work.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:29:48",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1788s",
      "context": "A blunt claim that challenges common safety assumptions and pushes teams to plan beyond guardrails as a primary defense.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t11_q1",
      "topic_id": "sander-schulhoff-20_t11",
      "topic_title": "Why guardrails fail in practice",
      "text": "So the first thing that we need to understand is that the number of possible attacks against another LLM is equivalent to the number of possible prompts. Each possible prompt could be an attack.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:31:04",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1864s",
      "context": "A core mental model: treat the prompt space itself as the attack surface, implying near-infinite adversarial variation.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t11_q2",
      "topic_id": "sander-schulhoff-20_t11",
      "topic_title": "Why guardrails fail in practice",
      "text": "99% of one followed by a million zeros, there's just so many attacks left. There's still basically infinite attacks left. And so, the number of attacks they're testing to get to that 99% figure is not statistically significant.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:32:07",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1927s",
      "context": "A practical critique of vendor metrics: even high pass rates can be meaningless when the underlying space is astronomically large and testing is undersampled.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t11_q3",
      "topic_id": "sander-schulhoff-20_t11",
      "topic_title": "Why guardrails fail in practice",
      "text": "And in fact, the best measurement you can do is an adaptive evaluation. And what that means is you take your defense, you take your model or your guardrail, and you build an attacker that can learn over time and improve its attacks.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:32:07",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=1927s",
      "context": "An actionable methodology: evaluate defenses with adaptive attackers rather than static test sets to better approximate real-world pressure.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t11_q4",
      "topic_id": "sander-schulhoff-20_t11",
      "topic_title": "Why guardrails fail in practice",
      "text": "And we found that, first of all, humans break everything. A hundred percent of the defenses in maybe like 10 to 30 attempts.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:33:25",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2005s",
      "context": "A concrete data point from competitive/adaptive testing that highlights how quickly real attackers can defeat \u201cstate-of-the-art\u201d guardrails.",
      "insight_type": "data"
    },
    {
      "quote_id": "sander-schulhoff-20_t11_q5",
      "topic_id": "sander-schulhoff-20_t11",
      "topic_title": "Why guardrails fail in practice",
      "text": "they tell me things like the testing we do is. They're fabricating statistics, and a lot of the times their models don't even work on non-English languages or something crazy like that, which is ridiculous because translating your attack to a different language is a very common attack pattern.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:35:44",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2144s",
      "context": "A practitioner warning: scrutinize evaluation integrity and multilingual coverage because simple translation is a common bypass technique.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t12_q1",
      "topic_id": "sander-schulhoff-20_t12",
      "topic_title": "Incentives: capabilities prioritized over security",
      "text": "The way he put it again is the only reason there hasn't been a massive attack is just how early adoption is, not because anything's actually secure.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:39:08",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2348s",
      "context": "A counterintuitive risk framing: the absence of major incidents may reflect low usage, not strong security\u2014useful for threat modeling and prioritization.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t12_q2",
      "topic_id": "sander-schulhoff-20_t12",
      "topic_title": "Incentives: capabilities prioritized over security",
      "text": "And then on the security side, it's like, or we can invest in security and they're more robust, but not smarter. And you have to have the intelligence first to be able to sell something. If you have something that's super secure but super dumb, it's worthless.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:40:17",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2417s",
      "context": "An incentives-based explanation practitioners can use to anticipate underinvestment in robustness and plan compensating controls outside the model.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t12_q3",
      "topic_id": "sander-schulhoff-20_t12",
      "topic_title": "Incentives: capabilities prioritized over security",
      "text": "you can patch a bug, but you can't patch a brain.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:41:56",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2516s",
      "context": "A memorable mental model for why AI security differs from classical cybersecurity and why \u201cfixing\u201d specific failures may not generalize.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t12_q4",
      "topic_id": "sander-schulhoff-20_t12",
      "topic_title": "Incentives: capabilities prioritized over security",
      "text": "Prompt-based defenses are the worst of the worst defenses.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:42:57",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2577s",
      "context": "Actionable guidance: don\u2019t rely on prompt engineering as a primary security control against adversarial users.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t12_q5",
      "topic_id": "sander-schulhoff-20_t12",
      "topic_title": "Incentives: capabilities prioritized over security",
      "text": "I guess to summarize again, automated red teaming works too well. It always works on any transformer-based or transformer-adjacent system, and guardrails work too poorly. They just don't work.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:43:28",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2608s",
      "context": "A practical methodology takeaway: assume automated red-teaming will find jailbreaks and treat guardrails as insufficient without additional layers.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t13_q1",
      "topic_id": "sander-schulhoff-20_t13",
      "topic_title": "Sponsor Break",
      "text": "It's basically your own mini foundation without the lawyers or admin costs.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:43:42",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2622s",
      "context": "Memorable framing that simplifies what a donor-advised fund is and why it\u2019s practical.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t13_q2",
      "topic_id": "sander-schulhoff-20_t13",
      "topic_title": "Sponsor Break",
      "text": "You contribute money or appreciated assets like stocks, get the tax deduction right away, potentially reduce capital gains, and then decide later where you want to donate.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:43:42",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2622s",
      "context": "Actionable description of how to use a DAF to separate tax timing from donation decisions.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t13_q3",
      "topic_id": "sander-schulhoff-20_t13",
      "topic_title": "Sponsor Break",
      "text": "There are zero admin or asset fees, and you can lock in your deductions now and decide where to give later, which is perfect for year-end giving.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:43:42",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2622s",
      "context": "Concrete cost/timing details that make the product\u2019s value proposition operational for year-end planning.",
      "insight_type": "data"
    },
    {
      "quote_id": "sander-schulhoff-20_t13_q4",
      "topic_id": "sander-schulhoff-20_t13",
      "topic_title": "Sponsor Break",
      "text": "I think we've done an excellent job helping people see the problem, get a little scared, see that there's not a silver bullet solution, that this is something that we really have to take seriously, and we're just lucky this hasn't been a huge problem yet.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:44:44",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2684s",
      "context": "Contrarian reminder that there\u2019s no single fix and that current safety may be more luck than readiness.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t14_q1",
      "topic_id": "sander-schulhoff-20_t14",
      "topic_title": "Practical mitigations and permissioning mindset",
      "text": "So again, to summarize there, any data that AI has access to, the user can make it leak it. Any actions that it can possibly take, the user can make it take. So make sure to have those things locked down.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:48:49",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2929s",
      "context": "A crisp permissioning mental model: assume users can coerce the model into leaking any accessible data or taking any available action, so minimize and lock down both.",
      "insight_type": "framework"
    },
    {
      "quote_id": "sander-schulhoff-20_t14_q2",
      "topic_id": "sander-schulhoff-20_t14",
      "topic_title": "Practical mitigations and permissioning mindset",
      "text": "So if you're just deploying chatbots and simple things that they don't really take actions or search the internet and they only have access to the user who's interacting with them's data, you're kind of fine.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:46:24",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2784s",
      "context": "Counterintuitive risk triage: basic FAQ-style chatbots without external actions or cross-user data access are relatively low risk compared to agentic systems.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "sander-schulhoff-20_t14_q3",
      "topic_id": "sander-schulhoff-20_t14",
      "topic_title": "Practical mitigations and permissioning mindset",
      "text": "So if there is some possible way for it to chain actions together in a way that becomes malicious, a user can make that happen.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:47:07",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=2827s",
      "context": "Actionable warning for agent design: if the system can take actions, assume adversaries can force harmful action chaining unless you constrain capabilities.",
      "insight_type": "advice"
    },
    {
      "quote_id": "sander-schulhoff-20_t14_q4",
      "topic_id": "sander-schulhoff-20_t14",
      "topic_title": "Practical mitigations and permissioning mindset",
      "text": "So in that case, prompt injection, completely solved, no problem.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:53:22",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=3202s",
      "context": "A concrete mitigation example: isolate model-generated code execution (e.g., containerize/sandbox) so even malicious outputs can\u2019t compromise the main application.",
      "insight_type": "story"
    },
    {
      "quote_id": "sander-schulhoff-20_t14_q5",
      "topic_id": "sander-schulhoff-20_t14",
      "topic_title": "Practical mitigations and permissioning mindset",
      "text": "So I definitely recommend having an AI security researcher or someone very, very familiar and who understands AI on your team.",
      "speaker": "Sander Schulhoff",
      "timestamp": "00:51:04",
      "youtube_link": "https://www.youtube.com/watch?v=J9982NLmTXg&t=3064s",
      "context": "Practical org guidance: dedicated in-house AI security expertise helps cut through misinformation and catch AI-specific failure modes classical security may miss.",
      "insight_type": "advice"
    }
  ]
}