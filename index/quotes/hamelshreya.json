{
  "episode_id": "hamelshreya",
  "quotes": [
    {
      "quote_id": "hamelshreya_t1_q1",
      "topic_id": "hamelshreya_t1",
      "topic_title": "Introduction and background: why evals matter",
      "text": "To build great AI products, you need to be really good at building evals. It's the highest ROI activity you can engage in.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:00:00",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=0s",
      "context": "Frames eval-building as the single most leveraged activity for AI product quality and iteration speed.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t1_q2",
      "topic_id": "hamelshreya_t1",
      "topic_title": "Introduction and background: why evals matter",
      "text": "The goal is not to do evals perfectly, it's to actionably improve your product.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:00:18",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=18s",
      "context": "A practical north star that prevents teams from over-optimizing measurement instead of shipping improvements.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t1_q3",
      "topic_id": "hamelshreya_t1",
      "topic_title": "Introduction and background: why evals matter",
      "text": "The top one is, \"We live in the age of AI. Can't the AI just eval it?\" But it doesn't work.",
      "speaker": "Hamel Husain",
      "timestamp": "00:00:39",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=39s",
      "context": "Counters the common assumption that evaluation can be fully automated by the same AI being tested.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t1_q4",
      "topic_id": "hamelshreya_t1",
      "topic_title": "Introduction and background: why evals matter",
      "text": "You can appoint one person whose taste that you trust. It should be the person with domain expertise. Oftentimes, it is the product manager.",
      "speaker": "Hamel Husain",
      "timestamp": "00:00:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=49s",
      "context": "Introduces a lightweight governance model (\u201cbenevolent dictator\u201d) to avoid committee drag and keep evals affordable.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t1_q5",
      "topic_id": "hamelshreya_t1",
      "topic_title": "Introduction and background: why evals matter",
      "text": "They've now taught over 2,000 PMs and engineers across 500 companies, including large swaths of the OpenAI and Anthropic teams along with every other major AI lab.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:01:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=101s",
      "context": "A concrete adoption signal that evals have become a mainstream capability across leading AI organizations.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t2_q1",
      "topic_id": "hamelshreya_t2",
      "topic_title": "Sponsor Break",
      "text": "Fin is the highest-performing AI agent on the market with a 65% average resolution rate.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=186s",
      "context": "Provides a concrete performance benchmark practitioners can use when evaluating or comparing AI support agents.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t2_q2",
      "topic_id": "hamelshreya_t2",
      "topic_title": "Sponsor Break",
      "text": "Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:18",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=198s",
      "context": "Highlights an adoption strategy\u2014reduce switching costs by integrating without migration\u2014to speed rollout and minimize operational risk.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t2_q3",
      "topic_id": "hamelshreya_t2",
      "topic_title": "Sponsor Break",
      "text": "Fin is trusted by over 5,000 customer service leaders and top AI companies like Anthropic and Synthesia.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:31",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=211s",
      "context": "Offers a specific social-proof data point and examples that can inform vendor due diligence.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t2_q4",
      "topic_id": "hamelshreya_t2",
      "topic_title": "Sponsor Break",
      "text": "And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:39",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=219s",
      "context": "Describes an iterative improvement loop (analyze \u2192 train \u2192 test \u2192 deploy) that teams can mirror for operating AI systems in production.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t2_q5",
      "topic_id": "hamelshreya_t2",
      "topic_title": "Sponsor Break",
      "text": "Whether you're running usability tests, interviews, surveys, or in-the-wild fieldwork, Dscout makes it easy to connect with real users and get real insights fast.",
      "speaker": "Sponsor (Dscout ad)",
      "timestamp": "00:04:17",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=257s",
      "context": "Emphasizes a practical research operating model: use multiple methods while optimizing for speed-to-insight with real users.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t3_q1",
      "topic_id": "hamelshreya_t3",
      "topic_title": "What evals are: definitions and scope",
      "text": "Evals is a way to systematically measure and improve an AI application, and it really doesn't have to be scary or unapproachable at all.",
      "speaker": "Hamel Husain",
      "timestamp": "00:05:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=349s",
      "context": "Defines evals as an approachable, repeatable discipline for improving LLM apps\u2014not an esoteric research practice.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t3_q2",
      "topic_id": "hamelshreya_t3",
      "topic_title": "What evals are: definitions and scope",
      "text": "It really is, at its core, data analytics on your LLM application and a systematic way of looking at that data, and where necessary, creating metrics around things so you can measure what's happening, and then so you can iterate and do experiments and improve.",
      "speaker": "Hamel Husain",
      "timestamp": "00:05:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=349s",
      "context": "Provides a concrete methodology: treat evals as analytics + metrics + iteration loops to drive experiments and improvement.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t3_q3",
      "topic_id": "hamelshreya_t3",
      "topic_title": "What evals are: definitions and scope",
      "text": "And before evals, you would be left with guessing. You would maybe fix a prompt and hope that you're not breaking anything else with that prompt, and you might rely on vibe checks, which is totally fine.",
      "speaker": "Hamel Husain",
      "timestamp": "00:06:36",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=396s",
      "context": "Highlights the failure mode of prompt-tweaking without measurement and why evals reduce regressions and uncertainty.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t3_q4",
      "topic_id": "hamelshreya_t3",
      "topic_title": "What evals are: definitions and scope",
      "text": "And vibe checks are good and you should do vibe checks initially, but it can become very unmanageable very fast because as your application grows, it's really hard to rely on vibe checks.",
      "speaker": "Hamel Husain",
      "timestamp": "00:07:11",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=431s",
      "context": "Actionable guidance on when to move from qualitative checks to systematic evals as product complexity scales.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t3_q5",
      "topic_id": "hamelshreya_t3",
      "topic_title": "What evals are: definitions and scope",
      "text": "So I would say, overall, unit tests are a very small part of that very big puzzle.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:09:24",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=564s",
      "context": "Reframes evals as broader than \u201cLLM unit tests,\u201d encompassing monitoring, cohorts, and ongoing quality metrics.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t4_q1",
      "topic_id": "hamelshreya_t4",
      "topic_title": "Setting up a real-world eval example (Nurture Boss)",
      "text": "it's really important that we don't think of evals as just tests.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Reframes evals as broader than traditional testing, setting up a more effective approach for LLM product work.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t4_q2",
      "topic_id": "hamelshreya_t4",
      "topic_title": "Setting up a real-world eval example (Nurture Boss)",
      "text": "There's a common trap that a lot of people fall into because they jump straight to the test like, \"Let me write some tests,\" and usually that's not what you want to do.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Warns practitioners against prematurely writing tests before understanding real failure modes.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t4_q3",
      "topic_id": "hamelshreya_t4",
      "topic_title": "Setting up a real-world eval example (Nurture Boss)",
      "text": "You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Offers a methodology: begin with data-driven error analysis rather than assumptions from conventional software engineering.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t4_q4",
      "topic_id": "hamelshreya_t4",
      "topic_title": "Setting up a real-world eval example (Nurture Boss)",
      "text": "So there's lots of different channels that you can interact through the AI with like chat, text, voice, but also, there's tool calls, lots of tool calls for booking appointments, getting information about availability, so on and so forth. There's also RAG retrieval, getting information about customers and properties and things like that.",
      "speaker": "Hamel Husain",
      "timestamp": "00:11:40",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=700s",
      "context": "Concrete example of why real-world AI apps need richer evaluation/observability due to multi-channel, tool-using, RAG-heavy complexity.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamelshreya_t4_q5",
      "topic_id": "hamelshreya_t4",
      "topic_title": "Setting up a real-world eval example (Nurture Boss)",
      "text": "So it's called a trace, and it's just the engineering term for logs of a sequence of events. The concept of a trace has been around for a really long time, but it's especially really important when it comes to AI applications.",
      "speaker": "Hamel Husain",
      "timestamp": "00:13:29",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=809s",
      "context": "Introduces traces as a key observability primitive for diagnosing and evaluating AI application behavior end-to-end.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t5_q1",
      "topic_id": "hamelshreya_t5",
      "topic_title": "Reading traces and writing open-coded notes",
      "text": "So it turns out there is a way to do it that is completely manageable, and it's not something that we invented. It's been around in machine learning and data science for a really long time, and it's called error analysis. And what you do is, the first step in conquering data like this is just to write notes. Okay?",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Introduces a proven methodology (error analysis) and the concrete first step (open-coded notes) for making messy LLM logs tractable.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t5_q2",
      "topic_id": "hamelshreya_t5",
      "topic_title": "Reading traces and writing open-coded notes",
      "text": "So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this. Usually a developer is not suited to do this, especially if it's not a coding application.",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Clarifies role ownership: evaluating traces is fundamentally product/UX judgment, not just a developer debugging task.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t5_q3",
      "topic_id": "hamelshreya_t5",
      "topic_title": "Reading traces and writing open-coded notes",
      "text": "A lot of people would say, \"Oh, it's great. The AI did the right thing. It looked, it said, 'We didn't have available,' and it's not available.\" But with your product hat on, you know that's not correct.",
      "speaker": "Hamel Husain",
      "timestamp": "00:18:32",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1112s",
      "context": "Challenges the \u201ctechnically correct response\u201d mindset by emphasizing product outcomes (e.g., lead management) over literal correctness.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t5_q4",
      "topic_id": "hamelshreya_t5",
      "topic_title": "Reading traces and writing open-coded notes",
      "text": "Everyone that does this immediately gets addicted to it and they say, \"This is the greatest thing that you can do when you're building an AI application.\" You just learn a lot and you're like, \"Hmm, this is not how I want it to work. Okay.\"",
      "speaker": "Hamel Husain",
      "timestamp": "00:19:30",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1170s",
      "context": "Highlights the high learning ROI of sampling traces and writing quick notes as an iterative product-building practice.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t5_q5",
      "topic_id": "hamelshreya_t5",
      "topic_title": "Reading traces and writing open-coded notes",
      "text": "The answer is, just write down the first thing that you see that's wrong, the most upstream error. Don't worry about all the errors, just capture the first thing that you see that's wrong, and stop, and move on.",
      "speaker": "Hamel Husain",
      "timestamp": "00:21:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1316s",
      "context": "Provides a practical heuristic for scalable manual review: focus on the earliest/root failure and keep moving to build signal quickly.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t6_q1",
      "topic_id": "hamelshreya_t6",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "what we usually find when we try to ask an LLM to do this error analysis is it just says the trace looks good because it doesn't have the context needed to understand whether something might be bad product smell or not.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:04",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1444s",
      "context": "Explains why LLMs fail at early-stage error analysis: they lack product context to detect subtle but important issues.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t6_q2",
      "topic_id": "hamelshreya_t6",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "I can guarantee you, I would bet money on this, if I put that into chat GPT and asked, \"Is there an error?\" it would say, \"No, did a great job.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:04",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1444s",
      "context": "A concrete example showing how LLMs can confidently miss errors during evaluation without domain/product grounding.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamelshreya_t6_q3",
      "topic_id": "hamelshreya_t6",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "But Hamel had the context of knowing, \"Oh, we don't actually have this virtual tour functionality,\" right? So I think, in these cases, it's so important to make sure you are manually doing this yourself.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:34",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1474s",
      "context": "Actionable guidance: manual review is required when correctness depends on product capabilities and constraints.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t6_q4",
      "topic_id": "hamelshreya_t6",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "But right now, in this stage of free form, note-taking is not the place for an LLM.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:58",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1498s",
      "context": "Defines a clear boundary for tool use: avoid LLM automation during open-ended early error analysis.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t7_q1",
      "topic_id": "hamelshreya_t7",
      "topic_title": "Benevolent dictator: avoiding committee paralysis",
      "text": "And so benevolent dictator is just a catchy term for the fact that when you're doing this open coding, a lot of teams get bogged down in having a committee do this.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "Defines the \u201cbenevolent dictator\u201d model as an antidote to committee-driven slowdown in evaluation/open-coding work.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t7_q2",
      "topic_id": "hamelshreya_t7",
      "topic_title": "Benevolent dictator: avoiding committee paralysis",
      "text": "You need to cut through the noise.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "A crisp directive to prioritize speed and decisiveness over broad consensus when setting evaluation criteria.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t7_q3",
      "topic_id": "hamelshreya_t7",
      "topic_title": "Benevolent dictator: avoiding committee paralysis",
      "text": "You don't want to make this process so expensive that you can't do it. You're going to lose out.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "Highlights the practical risk of over-engineering eval processes: if it\u2019s too costly, it won\u2019t happen and you fall behind.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t7_q4",
      "topic_id": "hamelshreya_t7",
      "topic_title": "Benevolent dictator: avoiding committee paralysis",
      "text": "It should be the person with domain expertise.",
      "speaker": "Hamel Husain",
      "timestamp": "00:27:16",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1636s",
      "context": "Actionable guidance on who should own the \u201cdictator\u201d role: the trusted domain expert who can judge correctness and relevance.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t7_q5",
      "topic_id": "hamelshreya_t7",
      "topic_title": "Benevolent dictator: avoiding committee paralysis",
      "text": "Though oftentimes, it is the product manager.",
      "speaker": "Hamel Husain",
      "timestamp": "00:27:42",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1662s",
      "context": "A concrete, common staffing pattern: PMs frequently serve as the single-threaded owner for open coding and eval decisions.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t8_q1",
      "topic_id": "hamelshreya_t8",
      "topic_title": "More trace examples and upstream-error focus",
      "text": "You could see the messiness of the real world in here, and the assistant just calls a tool that says transfer call, but it doesn't say anything. It just abruptly does transfer call, so it's pretty jank, I would say. It's just not-",
      "speaker": "Hamel Husain",
      "timestamp": "00:28:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1736s",
      "context": "Highlights a concrete real-world failure mode (silent tool invocation/abrupt transfer) that eval traces should capture.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamelshreya_t8_q2",
      "topic_id": "hamelshreya_t8",
      "topic_title": "More trace examples and upstream-error focus",
      "text": "So when you write the open note, you don't want to say jank, because what we want to do is we want to understand, and when we look at the notes later on, we want to understand what happened.",
      "speaker": "Hamel Husain",
      "timestamp": "00:29:14",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1754s",
      "context": "Gives a practical methodology for writing eval notes: describe observable behavior, not vague judgments.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t8_q3",
      "topic_id": "hamelshreya_t8",
      "topic_title": "More trace examples and upstream-error focus",
      "text": "So you just want to say, \"Did not confirm call transfer with user.\" And it doesn't have to be perfect. You just have to have a general idea of what's going on.",
      "speaker": "Hamel Husain",
      "timestamp": "00:29:24",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1764s",
      "context": "Provides an actionable template for turning messy qualitative issues into specific, reviewable eval annotations.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t8_q4",
      "topic_id": "hamelshreya_t8",
      "topic_title": "More trace examples and upstream-error focus",
      "text": "So, okay. So let's say we do, and Shreya and I, we recommend doing at least 100 of these.",
      "speaker": "Hamel Husain",
      "timestamp": "00:29:39",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1779s",
      "context": "Offers a concrete starting benchmark for how many traces to review to build intuition and surface issues.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t8_q5",
      "topic_id": "hamelshreya_t8",
      "topic_title": "More trace examples and upstream-error focus",
      "text": "So we just say 100 to mentally unblock you, so it's not intimidating. It's like, \"Don't worry, you're only going to do 100.\"",
      "speaker": "Hamel Husain",
      "timestamp": "00:30:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1807s",
      "context": "Shares a counterintuitive motivational tactic: use an arbitrary quota to reduce intimidation and get started.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t9_q1",
      "topic_id": "hamelshreya_t9",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So we just say 100 to mentally unblock you, so it's not intimidating. It's like, \"Don't worry, you're only going to do 100.\"",
      "speaker": "Hamel Husain",
      "timestamp": "00:30:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1807s",
      "context": "A practical heuristic: use \u201c100 traces\u201d as a motivational on-ramp to start doing evals instead of getting stuck.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t9_q2",
      "topic_id": "hamelshreya_t9",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "Keep looking at traces until you feel like you're not learning anything new.",
      "speaker": "Hamel Husain",
      "timestamp": "00:30:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1807s",
      "context": "Defines the real stopping rule for trace review as learning saturation rather than a fixed sample size.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t9_q3",
      "topic_id": "hamelshreya_t9",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So what this means is when you do all of these processes of looking at your data, when do you stop? It's when you are theoretically saturating or you're not uncovering any new types of notes, new types of concepts, or nothing that will materially change the next part of your process.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:30:31",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1831s",
      "context": "Introduces \u201ctheoretical saturation\u201d as a qualitative-analysis methodology for deciding when additional trace review stops paying off.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t9_q4",
      "topic_id": "hamelshreya_t9",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "A lot of people realize, \"Oh, okay. I only need to do 40, I only need to do 60. Actually, I only need to do 15.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:30:57",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1857s",
      "context": "Counterintuitive data point: many teams can stop far earlier than 100 once they build intuition for error analysis.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t9_q5",
      "topic_id": "hamelshreya_t9",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "And promise, at some point, you're not going to discover new types of problems.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:31:34",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1894s",
      "context": "Reassures practitioners that failure modes converge, reinforcing saturation as a reliable stopping condition.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t10_q1",
      "topic_id": "hamelshreya_t10",
      "topic_title": "From open codes to axial codes with LLM help",
      "text": "So basic counting is the most powerful analytical technique in data science because it's so simple and it's kind of undervalued in many cases, and so it's very approachable for people.",
      "speaker": "Hamel Husain",
      "timestamp": "00:32:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1926s",
      "context": "A contrarian reminder that simple frequency counts can drive high-leverage analysis before complex methods.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t10_q2",
      "topic_id": "hamelshreya_t10",
      "topic_title": "From open codes to axial codes with LLM help",
      "text": "You have a mess of open codes, and you don't have 100 distinct problems. Actually, many of them are repeats, but because you phrased them differently, and that you shouldn't have tried to create your taxonomy of failures as you're open coding. You just want to get down what's wrong and then organize, \"Okay, what's the most common failure mode?\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:33:54",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2034s",
      "context": "A practical methodology: separate capture (open coding) from organization (axial coding) to avoid premature taxonomy design.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t10_q3",
      "topic_id": "hamelshreya_t10",
      "topic_title": "From open codes to axial codes with LLM help",
      "text": "So the purpose, axial code basically is just a failure mode. It's the label or category. And what our goal is, is to get to this clusters of failure modes and figure out what is the most prevalent, so then you can go and run and attack that problem.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:34:19",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2059s",
      "context": "Defines axial codes as actionable failure modes and ties them directly to prioritization by prevalence.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t10_q4",
      "topic_id": "hamelshreya_t10",
      "topic_title": "From open codes to axial codes with LLM help",
      "text": "Note that it's not automatically proposing fixes or anything, that's our job, but now, we can wade through this mess of open codes a lot easier.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:35:53",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2153s",
      "context": "Clarifies the human/LLM division of labor: use LLMs for synthesis, keep solution design with practitioners.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t10_q5",
      "topic_id": "hamelshreya_t10",
      "topic_title": "From open codes to axial codes with LLM help",
      "text": "Another thing that's interesting here in this prompt to generate the axial codes is you can be very detailed if you want, right? You can say, \"I want each axial code to actually be some actionable failure mode,\" and maybe the LLM will understand that and propose it, or, \"I want you to group these open codes by what stage of the user story that it's in.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:36:05",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2165s",
      "context": "Gives concrete prompt patterns for tailoring axial-code clustering to be actionable or aligned to the user journey.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t11_q1",
      "topic_id": "hamelshreya_t11",
      "topic_title": "Grounding in prior art: error analysis isn\u2019t new",
      "text": "Well, I want to caveat that we didn't invent error analysis. We don't actually want to invent things. That's bad signal. If somebody is coming to you with a way to do something that's entirely new and not grounded in hundreds of years of theory and literature, then you should, I don't know, be a little bit wary of that.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:37:23",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2243s",
      "context": "A contrarian heuristic for evaluating new AI/LLM methodologies: prefer approaches grounded in established theory over \u201ctotally new\u201d claims.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t11_q2",
      "topic_id": "hamelshreya_t11",
      "topic_title": "Grounding in prior art: error analysis isn\u2019t new",
      "text": "But what we tried to do was distill, \"Okay, what are the new tools and techniques that you need to make sense of the LLM error-out analysis?\" And then we created a curriculum or structured way of doing this.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:37:42",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2262s",
      "context": "Frames their contribution as a structured, teachable process for LLM error analysis rather than a novel invention.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t11_q3",
      "topic_id": "hamelshreya_t11",
      "topic_title": "Grounding in prior art: error analysis isn\u2019t new",
      "text": "So this is all very tailored to LLMs, but the terms open coding, axial coding, are grounded in social science.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:37:42",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2262s",
      "context": "Points practitioners to specific, established qualitative analysis methods (open/axial coding) as a foundation for categorizing LLM errors.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t11_q4",
      "topic_id": "hamelshreya_t11",
      "topic_title": "Grounding in prior art: error analysis isn\u2019t new",
      "text": "And so this is a technique that's been used to analyze stochastic systems for ages, and it's something that it was just using the same machine learning ideas and principles, just bringing them into here, because again, these are stochastic systems.",
      "speaker": "Hamel Husain",
      "timestamp": "00:38:45",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2325s",
      "context": "A mental model for LLM products: treat them as stochastic systems and apply classic ML error analysis principles accordingly.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t11_q5",
      "topic_id": "hamelshreya_t11",
      "topic_title": "Grounding in prior art: error analysis isn\u2019t new",
      "text": "Okay. So you can do this through anything, and the same thing works just fine in ChatGPT, the same exact prompt. You can see it made axial codes. I really like using Julius AI. It's one of my favorite tools.",
      "speaker": "Hamel Husain",
      "timestamp": "00:39:31",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2371s",
      "context": "Practical tooling advice: the workflow is portable across tools (e.g., ChatGPT) and can generate axial codes with a consistent prompt.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t12_q1",
      "topic_id": "hamelshreya_t12",
      "topic_title": "Auto-labeling notes and counting failure modes",
      "text": "And I look at the correspondence between the different axial codes and the open codes, and I go through an exercise and I say, \"Hmm. Do I like these codes? Can I make them better? Can I refine them? Can I make them more specific?\" Instead of being generic, I make them very specific and actionable.",
      "speaker": "Hamel Husain",
      "timestamp": "00:40:10",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2410s",
      "context": "A practical methodology for iterating on AI-suggested categories so they become actionable rather than vague.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t12_q2",
      "topic_id": "hamelshreya_t12",
      "topic_title": "Auto-labeling notes and counting failure modes",
      "text": "So this also drives home the point that your open codes have to be detailed, right? You can't just say janky because if the AI is reading janky, it's not going to be able to categorize it. Even a human wouldn't, right?",
      "speaker": "Shreya Shankar",
      "timestamp": "00:42:22",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2542s",
      "context": "Concrete guidance on writing better labels/notes so auto-labeling works reliably for both AI and humans.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t12_q3",
      "topic_id": "hamelshreya_t12",
      "topic_title": "Auto-labeling notes and counting failure modes",
      "text": "One of the things that I like to do with this step if I'm trying to use AI to do this labeling, is also have a new category called none of the above. So an AI can actually say, \"None of the above,\" in the axial code, and that informs me, \"Okay, my axial codes are not complete. Let's go look at those open codes, let's figure out what some new categories are or figure out how to reword my other axial codes.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:43:34",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2614s",
      "context": "A simple but powerful \u201cnone of the above\u201d mechanism to detect taxonomy gaps and drive continuous refinement.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t12_q4",
      "topic_id": "hamelshreya_t12",
      "topic_title": "Auto-labeling notes and counting failure modes",
      "text": "So here's a pivot table, and we just can do pivot table on those, and we can count how many times those different things occurred. So what do we find? Find on these traces that we categorized? We found 17 conversational flow issues.",
      "speaker": "Hamel Husain",
      "timestamp": "00:44:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2696s",
      "context": "Shows the quantification step\u2014turning labeled failure modes into counts to prioritize what to fix.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamelshreya_t12_q5",
      "topic_id": "hamelshreya_t12",
      "topic_title": "Auto-labeling notes and counting failure modes",
      "text": "It's not necessarily the count is the most important thing. It might be something that's just really bad and you want to fix that, but okay. Now, you have some way of looking at your problem, and now you can think about whether you need evals for some of these.",
      "speaker": "Hamel Husain",
      "timestamp": "00:45:25",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2725s",
      "context": "A counterbalance to pure frequency-based prioritization, tying analysis to the decision of where evals are actually needed.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t13_q1",
      "topic_id": "hamelshreya_t13",
      "topic_title": "Choosing what to eval and eval types preview",
      "text": "So there might be some of these things that might be just dumb engineering errors that you don't need to write an eval for because it's very obvious on how to fix them.",
      "speaker": "Hamel+Shreya",
      "timestamp": "00:46:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2767s",
      "context": "A reminder to triage issues first\u2014some failures are straightforward engineering/prompt fixes and don\u2019t merit the overhead of an eval.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamelshreya_t13_q2",
      "topic_id": "hamelshreya_t13",
      "topic_title": "Choosing what to eval and eval types preview",
      "text": "You might still want to write an eval for that because you might be able to test that with just code. You could just test the string, does it have the right formatting potentially? Without running an LLM.",
      "speaker": "Hamel+Shreya",
      "timestamp": "00:46:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2767s",
      "context": "Concrete example of preferring cheap, deterministic checks (string/format validation) over expensive model-in-the-loop evaluation.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t13_q3",
      "topic_id": "hamelshreya_t13",
      "topic_title": "Choosing what to eval and eval types preview",
      "text": "So there's a cost-benefit trade-off to evals. You don't want to get carried away with it, but you want to usually ground yourself in your actual errors. You don't want to skip this step.",
      "speaker": "Hamel+Shreya",
      "timestamp": "00:46:53",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2813s",
      "context": "A practical mental model: treat evals as an investment decision and anchor them to observed failure modes rather than abstract test-writing.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamelshreya_t13_q4",
      "topic_id": "hamelshreya_t13",
      "topic_title": "Choosing what to eval and eval types preview",
      "text": "They go straight into evals like, \"Let me just write some tests,\" and that is where things go off the rails.",
      "speaker": "Hamel+Shreya",
      "timestamp": "00:46:53",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2813s",
      "context": "Counterintuitive warning that starting with tests (instead of diagnosing real errors) can derail evaluation efforts.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamelshreya_t13_q5",
      "topic_id": "hamelshreya_t13",
      "topic_title": "Choosing what to eval and eval types preview",
      "text": "One is code-based, which you should try to do if you can because they're cheaper. LLM as a judge is something, it's like a meta eval. You have to eval that eval to make sure the LLM that's judging is doing the right thing, which we'll talk about in a second.",
      "speaker": "Hamel+Shreya",
      "timestamp": "00:47:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2876s",
      "context": "A clear taxonomy plus a key operational implication: LLM-as-judge adds a second-order validation burden compared to cheaper code-based evals.",
      "insight_type": "framework"
    }
  ]
}