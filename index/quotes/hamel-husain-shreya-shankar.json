{
  "episode_id": "hamel-husain-shreya-shankar",
  "quotes": [
    {
      "quote_id": "hamel-husain-shreya-shankar_t1_q1",
      "topic_id": "hamel-husain-shreya-shankar_t1",
      "topic_title": "Introduction and Background",
      "text": "To build great AI products, you need to be really good at building evals. It's the highest ROI activity you can engage in.",
      "speaker": "Lenny Rachitsky",
      "timestamp": "00:00:00",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=0s",
      "context": "Frames eval-building as a disproportionately valuable investment for AI product teams.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t1_q2",
      "topic_id": "hamel-husain-shreya-shankar_t1",
      "topic_title": "Introduction and Background",
      "text": "The goal is not to do evals perfectly, it's to actionably improve your product.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:00:18",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=18s",
      "context": "Reorients evals away from academic rigor toward iterative product improvement.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t1_q3",
      "topic_id": "hamel-husain-shreya-shankar_t1",
      "topic_title": "Introduction and Background",
      "text": "The top one is, \"We live in the age of AI. Can't the AI just eval it?\" But it doesn't work.",
      "speaker": "Hamel Husain",
      "timestamp": "00:00:39",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=39s",
      "context": "Challenges the common assumption that LLMs can reliably replace human-designed evaluation.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t1_q4",
      "topic_id": "hamel-husain-shreya-shankar_t1",
      "topic_title": "Introduction and Background",
      "text": "You can appoint one person whose taste that you trust. It should be the person with domain expertise. Oftentimes, it is the product manager.",
      "speaker": "Hamel Husain",
      "timestamp": "00:00:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=49s",
      "context": "Offers a practical operating model (\u201cbenevolent dictator\u201d) to avoid committee gridlock in eval design.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t2_q1",
      "topic_id": "hamel-husain-shreya-shankar_t2",
      "topic_title": "Sponsor Break",
      "text": "Fin is the highest-performing AI agent on the market with a 65% average resolution rate.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=187s",
      "context": "Concrete performance benchmark practitioners can use to evaluate or compare AI support agents.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t2_q2",
      "topic_id": "hamel-husain-shreya-shankar_t2",
      "topic_title": "Sponsor Break",
      "text": "Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:20",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=200s",
      "context": "Actionable adoption principle: reduce integration/migration friction to de-risk tool switching.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t2_q3",
      "topic_id": "hamel-husain-shreya-shankar_t2",
      "topic_title": "Sponsor Break",
      "text": "And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:43",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=223s",
      "context": "Highlights an iterative methodology (analyze \u2192 train \u2192 test \u2192 deploy) for continuously improving AI system outcomes.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t2_q4",
      "topic_id": "hamel-husain-shreya-shankar_t2",
      "topic_title": "Sponsor Break",
      "text": "You can even test your Figma prototypes directly inside the platform.",
      "speaker": "Sponsor (Dscout ad)",
      "timestamp": "00:04:30",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=270s",
      "context": "Specific workflow example that speeds up research by embedding prototype testing into the research toolchain.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t2_q5",
      "topic_id": "hamel-husain-shreya-shankar_t2",
      "topic_title": "Sponsor Break",
      "text": "So if you're ready to transform your customer service and scale your support, give Fin a try for only 99 cents per resolution. Plus, Fin comes with a 90-day money-back guarantee.",
      "speaker": "Sponsor (Fin ad)",
      "timestamp": "00:03:58",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=238s",
      "context": "Pricing and risk-reversal details that can inform ROI calculations and procurement decisions.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t3_q1",
      "topic_id": "hamel-husain-shreya-shankar_t3",
      "topic_title": "What evals are and why they matter",
      "text": "Evals is a way to systematically measure and improve an AI application, and it really doesn't have to be scary or unapproachable at all.",
      "speaker": "Hamel Husain",
      "timestamp": "00:05:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=349s",
      "context": "Defines evals as a practical, repeatable discipline for improving LLM apps rather than an intimidating specialty.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t3_q2",
      "topic_id": "hamel-husain-shreya-shankar_t3",
      "topic_title": "What evals are and why they matter",
      "text": "It really is, at its core, data analytics on your LLM application and a systematic way of looking at that data, and where necessary, creating metrics around things so you can measure what's happening, and then so you can iterate and do experiments and improve.",
      "speaker": "Hamel Husain",
      "timestamp": "00:05:49",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=349s",
      "context": "Reframes evals as analytics + metrics + experimentation\u2014an actionable methodology teams can operationalize.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t3_q3",
      "topic_id": "hamel-husain-shreya-shankar_t3",
      "topic_title": "What evals are and why they matter",
      "text": "And before evals, you would be left with guessing. You would maybe fix a prompt and hope that you're not breaking anything else with that prompt, and you might rely on vibe checks, which is totally fine.",
      "speaker": "Hamel Husain",
      "timestamp": "00:06:36",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=396s",
      "context": "Highlights the common failure mode of prompt-tweaking without measurement, motivating the need for evals.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t3_q4",
      "topic_id": "hamel-husain-shreya-shankar_t3",
      "topic_title": "What evals are and why they matter",
      "text": "And vibe checks are good and you should do vibe checks initially, but it can become very unmanageable very fast because as your application grows, it's really hard to rely on vibe checks.",
      "speaker": "Hamel Husain",
      "timestamp": "00:07:11",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=431s",
      "context": "Gives a pragmatic rule: start with qualitative checks, but expect them to break down quickly at scale.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t3_q5",
      "topic_id": "hamel-husain-shreya-shankar_t3",
      "topic_title": "What evals are and why they matter",
      "text": "And so evals help you create metrics that you can use to measure how your application is doing and kind of give you a way to improve your application with confidence. That you have a feedback signal in which to iterate against.",
      "speaker": "Hamel Husain",
      "timestamp": "00:07:11",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=431s",
      "context": "Explains the core value proposition of evals: metrics as a feedback signal for confident iteration.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t4_q1",
      "topic_id": "hamel-husain-shreya-shankar_t4",
      "topic_title": "Evals spectrum vs unit tests metaphor",
      "text": "Evals is a big spectrum of ways to measure application quality.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:08:35",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=515s",
      "context": "Reframes evals from \u201ctests\u201d to a broader quality-measurement toolkit, expanding how practitioners should think about evaluation.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t4_q2",
      "topic_id": "hamel-husain-shreya-shankar_t4",
      "topic_title": "Evals spectrum vs unit tests metaphor",
      "text": "So evals could also be a way of looking at your data regularly to find these new cohorts of people.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:09:24",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=564s",
      "context": "Actionable guidance to use evals for ongoing cohort discovery and adaptation as new user groups emerge.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t4_q3",
      "topic_id": "hamel-husain-shreya-shankar_t4",
      "topic_title": "Evals spectrum vs unit tests metaphor",
      "text": "So I would say, overall, unit tests are a very small part of that very big puzzle.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:09:24",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=564s",
      "context": "Counteracts the common \u201cevals = unit tests\u201d framing and encourages broader evaluation strategies beyond strict correctness checks.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t4_q4",
      "topic_id": "hamel-husain-shreya-shankar_t4",
      "topic_title": "Evals spectrum vs unit tests metaphor",
      "text": "There's a common trap that a lot of people fall into because they jump straight to the test like, \"Let me write some tests,\" and usually that's not what you want to do.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Warns practitioners against prematurely writing tests and hints at a more effective evaluation workflow.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t4_q5",
      "topic_id": "hamel-husain-shreya-shankar_t4",
      "topic_title": "Evals spectrum vs unit tests metaphor",
      "text": "You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Provides a concrete methodology: begin with data analysis to decide what to evaluate, reflecting LLM uncertainty and broader surface area.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t5_q1",
      "topic_id": "hamel-husain-shreya-shankar_t5",
      "topic_title": "Real-world example setup: Nurture Boss traces",
      "text": "it's really important that we don't think of evals as just tests.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Reframes evals as broader than traditional testing, setting up a more effective approach for LLM product work.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t5_q2",
      "topic_id": "hamel-husain-shreya-shankar_t5",
      "topic_title": "Real-world example setup: Nurture Boss traces",
      "text": "There's a common trap that a lot of people fall into because they jump straight to the test like, \"Let me write some tests,\" and usually that's not what you want to do.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Warns against prematurely writing tests before understanding real failure modes in production.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t5_q3",
      "topic_id": "hamel-husain-shreya-shankar_t5",
      "topic_title": "Real-world example setup: Nurture Boss traces",
      "text": "You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work.",
      "speaker": "Hamel Husain",
      "timestamp": "00:10:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=606s",
      "context": "Gives a concrete methodology: begin eval work with data/error analysis rather than assumptions.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t5_q4",
      "topic_id": "hamel-husain-shreya-shankar_t5",
      "topic_title": "Real-world example setup: Nurture Boss traces",
      "text": "So let's go through the very beginning stage, what we call error analysis, which is, let's look at the data of their application and first start with what's going wrong.",
      "speaker": "Hamel Husain",
      "timestamp": "00:12:36",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=756s",
      "context": "Names and defines the first step of an eval workflow: error analysis grounded in real application data.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t5_q5",
      "topic_id": "hamel-husain-shreya-shankar_t5",
      "topic_title": "Real-world example setup: Nurture Boss traces",
      "text": "So it's called a trace, and it's just the engineering term for logs of a sequence of events.",
      "speaker": "Hamel Husain",
      "timestamp": "00:13:29",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=809s",
      "context": "Introduces \u201ctraces\u201d as the practical unit of analysis for inspecting real-world LLM app behavior.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t6_q1",
      "topic_id": "hamel-husain-shreya-shankar_t6",
      "topic_title": "Reading a trace: prompts, tools, and responses",
      "text": "Now, this is an example of a trace, and we're looking at one specific data point.",
      "speaker": "Hamel Husain",
      "timestamp": "00:16:25",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=985s",
      "context": "Frames a trace as a concrete unit of analysis\u2014one logged interaction you can inspect end-to-end.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t6_q2",
      "topic_id": "hamel-husain-shreya-shankar_t6",
      "topic_title": "Reading a trace: prompts, tools, and responses",
      "text": "So it turns out there is a way to do it that is completely manageable, and it's not something that we invented. It's been around in machine learning and data science for a really long time, and it's called error analysis.",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Introduces error analysis as the established methodology for making messy LLM logs analyzable.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t6_q3",
      "topic_id": "hamel-husain-shreya-shankar_t6",
      "topic_title": "Reading a trace: prompts, tools, and responses",
      "text": "And what you do is, the first step in conquering data like this is just to write notes. Okay?",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Gives a simple, actionable first step for practitioners: annotate traces before jumping to fixes or metrics.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t6_q4",
      "topic_id": "hamel-husain-shreya-shankar_t6",
      "topic_title": "Reading a trace: prompts, tools, and responses",
      "text": "So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this.",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Emphasizes that evaluating LLM behavior is product-context-dependent and can\u2019t be delegated purely to engineering.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t6_q5",
      "topic_id": "hamel-husain-shreya-shankar_t6",
      "topic_title": "Reading a trace: prompts, tools, and responses",
      "text": "Usually a developer is not suited to do this, especially if it's not a coding application.",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "A contrarian take that challenges the default assumption that developers should own evaluation for non-coding LLM products.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t7_q1",
      "topic_id": "hamel-husain-shreya-shankar_t7",
      "topic_title": "Open coding: manual error analysis with notes",
      "text": "And what you do is, the first step in conquering data like this is just to write notes. Okay? So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this.",
      "speaker": "Hamel Husain",
      "timestamp": "00:17:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1027s",
      "context": "Frames manual note-taking as the first, manageable step of error analysis\u2014and argues it requires product involvement, not just engineering.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t7_q2",
      "topic_id": "hamel-husain-shreya-shankar_t7",
      "topic_title": "Open coding: manual error analysis with notes",
      "text": "A lot of people would say, \"Oh, it's great. The AI did the right thing. It looked, it said, 'We didn't have available,' and it's not available.\" But with your product hat on, you know that's not correct.",
      "speaker": "Hamel Husain",
      "timestamp": "00:18:32",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1112s",
      "context": "Highlights a counterintuitive gap between \u201ctechnically correct\u201d model behavior and the desired product experience.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t7_q3",
      "topic_id": "hamel-husain-shreya-shankar_t7",
      "topic_title": "Open coding: manual error analysis with notes",
      "text": "Yeah, and you don't have to do it for all of your data. You sample your data and just take a look, and it's surprising how much you learn when you do this.",
      "speaker": "Hamel Husain",
      "timestamp": "00:19:30",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1170s",
      "context": "Gives a practical, scalable tactic: sample traces instead of reviewing everything, because learning per sample is high.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t7_q4",
      "topic_id": "hamel-husain-shreya-shankar_t7",
      "topic_title": "Open coding: manual error analysis with notes",
      "text": "Yeah, it's supposed to be chill. Just don't overthink it. And there's a way to do this. So the question always comes up, how do you do this? Do you try to find all the different problems in this trace? What do you write a note about? And the answer is, just write down the first thing that you see that's wrong, the most upstream error. Don't worry about all the errors, just capture the first thing that you see that's wrong, and stop, and move on.",
      "speaker": "Hamel Husain",
      "timestamp": "00:21:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1316s",
      "context": "Provides a concrete methodology for manual error analysis: keep notes informal and record only the first, most upstream error before moving on.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t7_q5",
      "topic_id": "hamel-husain-shreya-shankar_t7",
      "topic_title": "Open coding: manual error analysis with notes",
      "text": "So it is hallucinating something that doesn't exist.",
      "speaker": "Hamel Husain",
      "timestamp": "00:23:16",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1396s",
      "context": "A crisp example of a real failure mode (hallucinated product capability) that practitioners can label and track during evaluation.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t8_q1",
      "topic_id": "hamel-husain-shreya-shankar_t8",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "And I loved Hamel's most recent example because what we usually find when we try to ask an LLM to do this error analysis is it just says the trace looks good because it doesn't have the context needed to understand whether something might be bad product smell or not.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:04",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1444s",
      "context": "Explains why LLM-led error analysis can miss product/business-context failures and create false confidence.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t8_q2",
      "topic_id": "hamel-husain-shreya-shankar_t8",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "For example, the hallucination about scheduling the tour, right? I can guarantee you, I would bet money on this, if I put that into chat GPT and asked, \"Is there an error?\" it would say, \"No, did a great job.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:04",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1444s",
      "context": "Concrete example showing how hallucinations can look \u201cfine\u201d to an LLM without situational context.",
      "insight_type": "story"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t8_q3",
      "topic_id": "hamel-husain-shreya-shankar_t8",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "But Hamel had the context of knowing, \"Oh, we don't actually have this virtual tour functionality,\" right? So I think, in these cases, it's so important to make sure you are manually doing this yourself.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:34",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1474s",
      "context": "Actionable guidance: keep early-stage error analysis manual because only humans reliably know what the product can/can\u2019t do.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t8_q4",
      "topic_id": "hamel-husain-shreya-shankar_t8",
      "topic_title": "Why LLMs can\u2019t replace early error analysis",
      "text": "But right now, in this stage of free form, note-taking is not the place for an LLM.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:24:58",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1498s",
      "context": "Clear boundary-setting on where LLMs don\u2019t belong in the workflow (free-form open coding / note-taking).",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t9_q1",
      "topic_id": "hamel-husain-shreya-shankar_t9",
      "topic_title": "Benevolent dictator: avoid committee bottlenecks",
      "text": "And so benevolent dictator is just a catchy term for the fact that when you're doing this open coding, a lot of teams get bogged down in having a committee do this. And for a lot of situations, that's wholly unnecessary.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "A clear framing that committees often slow down eval/open-coding work unnecessarily, motivating a single-owner model.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t9_q2",
      "topic_id": "hamel-husain-shreya-shankar_t9",
      "topic_title": "Benevolent dictator: avoid committee bottlenecks",
      "text": "You need to cut through the noise. And a lot of organizations, if you look really deeply, especially small, medium-sized companies, you can appoint one person whose tastes that you trust.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "Actionable guidance to designate a trusted decision-maker to keep the process fast and coherent.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t9_q3",
      "topic_id": "hamel-husain-shreya-shankar_t9",
      "topic_title": "Benevolent dictator: avoid committee bottlenecks",
      "text": "And you can do this with a small number of people and often one person, and it's really important to make this tractable. You don't want to make this process so expensive that you can't do it. You're going to lose out.",
      "speaker": "Hamel Husain",
      "timestamp": "00:25:41",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1541s",
      "context": "A practical constraint: keep eval processes lightweight or they won\u2019t be sustainable enough to run.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t9_q4",
      "topic_id": "hamel-husain-shreya-shankar_t9",
      "topic_title": "Benevolent dictator: avoid committee bottlenecks",
      "text": "Another thing that we'll talk about later is when it goes to building an LLM as a judge, you need a binary score. You don't want to think about, \"Is this like a 1, 2, 3, 4, 5?\" Like, assign a score to it. You can't. That's going to slow it down.",
      "speaker": "Hamel Husain",
      "timestamp": "00:26:36",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1596s",
      "context": "A concrete methodology choice\u2014binary scoring\u2014to reduce ambiguity and speed up LLM-judge evaluation loops.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t9_q5",
      "topic_id": "hamel-husain-shreya-shankar_t9",
      "topic_title": "Benevolent dictator: avoid committee bottlenecks",
      "text": "Yeah. It's going to be okay. It's not perfection. You're just trying to make progress and get signal quickly so you have an idea of what to work on because it can become infinitely expensive if you're not careful.",
      "speaker": "Hamel Husain",
      "timestamp": "00:27:52",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1672s",
      "context": "A memorable reminder to prioritize fast signal over perfect consensus to avoid runaway evaluation costs.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t10_q1",
      "topic_id": "hamel-husain-shreya-shankar_t10",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So, okay. So let's say we do, and Shreya and I, we recommend doing at least 100 of these. The question is always, \"How many of this do you do?\" And so there's not a magic number. We say 100 just because we know that as soon as you start doing this, once you do 20 of these, you will automatically find it so useful that you will continue doing it.",
      "speaker": "Hamel Husain",
      "timestamp": "00:29:39",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1779s",
      "context": "A practical starting point (100 traces) framed as a motivational tool rather than a rigid requirement, with an observed tipping point around 20 reviews.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t10_q2",
      "topic_id": "hamel-husain-shreya-shankar_t10",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So we just say 100 to mentally unblock you, so it's not intimidating. It's like, \"Don't worry, you're only going to do 100.\" And there is a term for that, so the right answer is, \"Keep looking at traces until you feel like you're not learning anything new.\"",
      "speaker": "Hamel Husain",
      "timestamp": "00:30:07",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1807s",
      "context": "A simple stopping rule that prioritizes learning velocity over arbitrary sample sizes, while using \u201c100\u201d as a psychological commitment device.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t10_q3",
      "topic_id": "hamel-husain-shreya-shankar_t10",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So there's actually a term-... in data analysis and qualitative analysis called theoretical saturation. So what this means is when you do all of these processes of looking at your data, when do you stop? It's when you are theoretically saturating or you're not uncovering any new types of notes, new types of concepts, or nothing that will materially change the next part of your process.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:30:31",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1831s",
      "context": "Introduces \u201ctheoretical saturation\u201d as a qualitative methodology for deciding when to stop reviewing traces based on diminishing new insights.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t10_q4",
      "topic_id": "hamel-husain-shreya-shankar_t10",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "And this kind of takes a little bit of intuition to develop, so typically, people don't really know when they've reached theoretical saturation yet. That's totally fine. When you do two or three examples or rounds of this, you will develop the intuition. A lot of people realize, \"Oh, okay. I only need to do 40, I only need to do 60. Actually, I only need to do 15.\" I don't know. Depends on the application and depends on how savvy you are with error analysis for sure.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:30:57",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1857s",
      "context": "Normalizes uncertainty and emphasizes iterative calibration\u2014your needed sample size can be far smaller once you build intuition through a few rounds.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t10_q5",
      "topic_id": "hamel-husain-shreya-shankar_t10",
      "topic_title": "How many traces to review: theoretical saturation",
      "text": "So you just want to say, \"Did not confirm call transfer with user.\" And it doesn't have to be perfect. You just have to have a general idea of what's going on.",
      "speaker": "Hamel Husain",
      "timestamp": "00:29:24",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1764s",
      "context": "Guidance for writing evaluation notes: be specific and interpretable later, without over-optimizing for perfect wording.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t11_q1",
      "topic_id": "hamel-husain-shreya-shankar_t11",
      "topic_title": "Axial coding: using LLMs to cluster notes",
      "text": "So basic counting is the most powerful analytical technique in data science because it's so simple and it's kind of undervalued in many cases, and so it's very approachable for people.",
      "speaker": "Hamel Husain",
      "timestamp": "00:32:06",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=1926s",
      "context": "A practical reminder that simple frequency counts of failure notes often outperform more complex analysis for prioritizing what to fix.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t11_q2",
      "topic_id": "hamel-husain-shreya-shankar_t11",
      "topic_title": "Axial coding: using LLMs to cluster notes",
      "text": "You have a mess of open codes, and you don't have 100 distinct problems. Actually, many of them are repeats, but because you phrased them differently, and that you shouldn't have tried to create your taxonomy of failures as you're open coding. You just want to get down what's wrong and then organize, \"Okay, what's the most common failure mode?\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:33:54",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2034s",
      "context": "A clear methodology: capture raw issues first (open coding), then cluster later (axial coding) to find the dominant failure modes.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t11_q3",
      "topic_id": "hamel-husain-shreya-shankar_t11",
      "topic_title": "Axial coding: using LLMs to cluster notes",
      "text": "Now, do I like all the categories? Not really. I like some of them. It's a good first stab at it. I would probably rename it a little bit because some of them are a bit too generic. Like what is capability limitation? That's a little bit too broad. It's not actionable. I want to get a little bit more actionable with it so that if I do decide it's a problem, I know what to do with it, but we'll discuss that in a little bit.",
      "speaker": "Hamel Husain",
      "timestamp": "00:35:18",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2118s",
      "context": "Guidance for practitioners: treat LLM-generated clusters as a draft, then refine labels until they\u2019re specific enough to drive concrete fixes.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t11_q4",
      "topic_id": "hamel-husain-shreya-shankar_t11",
      "topic_title": "Axial coding: using LLMs to cluster notes",
      "text": "Another thing that's interesting here in this prompt to generate the axial codes is you can be very detailed if you want, right? You can say, \"I want each axial code to actually be some actionable failure mode,\" and maybe the LLM will understand that and propose it, or, \"I want you to group these open codes by what stage of the user story that it's in.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:36:05",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2165s",
      "context": "A reusable prompting framework: specify the clustering lens (actionability, user-journey stage, etc.) to produce categories that match how you\u2019ll execute improvements.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t11_q5",
      "topic_id": "hamel-husain-shreya-shankar_t11",
      "topic_title": "Axial coding: using LLMs to cluster notes",
      "text": "Well, I want to caveat that we didn't invent error analysis. We don't actually want to invent things. That's bad signal. If somebody is coming to you with a way to do something that's entirely new and not grounded in hundreds of years of theory and literature, then you should, I don't know, be a little bit wary of that.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:37:23",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2243s",
      "context": "A contrarian heuristic for evaluating AI \u201cbest practices\u201d: prefer methods grounded in established research over shiny, unvalidated novelty.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t12_q1",
      "topic_id": "hamel-husain-shreya-shankar_t12",
      "topic_title": "Auto-labeling and counting failures with spreadsheets",
      "text": "So this also drives home the point that your open codes have to be detailed, right? You can't just say janky because if the AI is reading janky, it's not going to be able to categorize it. Even a human wouldn't, right? It would have to go and remember why you said janky, so it's important to be somewhat detailed in your open code.",
      "speaker": "Shreya Shankar",
      "timestamp": "00:42:22",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2542s",
      "context": "Practical guidance on making labels usable for both humans and AI by writing sufficiently specific open codes.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t12_q2",
      "topic_id": "hamel-husain-shreya-shankar_t12",
      "topic_title": "Auto-labeling and counting failures with spreadsheets",
      "text": "Just like anything that AI does, it's really good to kind of put yourself in the middle just a little bit.",
      "speaker": "Hamel Husain",
      "timestamp": "00:43:17",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2597s",
      "context": "A simple mental model for keeping humans in the loop when iterating on AI-generated categorizations.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t12_q3",
      "topic_id": "hamel-husain-shreya-shankar_t12",
      "topic_title": "Auto-labeling and counting failures with spreadsheets",
      "text": "One of the things that I like to do with this step if I'm trying to use AI to do this labeling, is also have a new category called none of the above. So an AI can actually say, \"None of the above,\" in the axial code, and that informs me, \"Okay, my axial codes are not complete. Let's go look at those open codes, let's figure out what some new categories are or figure out how to reword my other axial codes.\"",
      "speaker": "Shreya Shankar",
      "timestamp": "00:43:34",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2614s",
      "context": "A concrete technique for detecting taxonomy gaps and continuously improving your category system.",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t12_q4",
      "topic_id": "hamel-husain-shreya-shankar_t12",
      "topic_title": "Auto-labeling and counting failures with spreadsheets",
      "text": "So here's a pivot table, and we just can do pivot table on those, and we can count how many times those different things occurred. So what do we find? Find on these traces that we categorized? We found 17 conversational flow issues.",
      "speaker": "Hamel Husain",
      "timestamp": "00:44:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2696s",
      "context": "Shows how to quantify top failure modes with a spreadsheet workflow, including an example count to prioritize fixes.",
      "insight_type": "data"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t12_q5",
      "topic_id": "hamel-husain-shreya-shankar_t12",
      "topic_title": "Auto-labeling and counting failures with spreadsheets",
      "text": "So there's a cost-benefit trade-off to evals. You don't want to get carried away with it, but you want to usually ground yourself in your actual errors. You don't want to skip this step. And so the reason I'm kind of spending so much time on this is this is where people get lost. They go straight into evals like, \"Let me just write some tests,\" and that is where things go off the rails.",
      "speaker": "Hamel Husain",
      "timestamp": "00:46:53",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2813s",
      "context": "A contrarian warning against jumping straight to evals without first analyzing real-world errors and prioritizing what matters.",
      "insight_type": "contrarian"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t13_q1",
      "topic_id": "hamel-husain-shreya-shankar_t13",
      "topic_title": "From error counts to eval types and LLM judges",
      "text": "Well, that might be sort of an interesting thing for an LLM as a judge, for example.",
      "speaker": "Hamel Husain",
      "timestamp": "00:47:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2876s",
      "context": "Highlights a practical heuristic: use an LLM judge when the evaluation is inherently subjective (e.g., whether to hand off to a human).",
      "insight_type": "framework"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t13_q2",
      "topic_id": "hamel-husain-shreya-shankar_t13",
      "topic_title": "From error counts to eval types and LLM judges",
      "text": "So there's different kinds of evals. One is code-based, which you should try to do if you can because they're cheaper.",
      "speaker": "Hamel Husain",
      "timestamp": "00:47:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2876s",
      "context": "Actionable guidance to prefer code-based evals first due to cost, reserving more expensive approaches for when needed.",
      "insight_type": "advice"
    },
    {
      "quote_id": "hamel-husain-shreya-shankar_t13_q3",
      "topic_id": "hamel-husain-shreya-shankar_t13",
      "topic_title": "From error counts to eval types and LLM judges",
      "text": "LLM as a judge is something, it's like a meta eval. You have to eval that eval to make sure the LLM that's judging is doing the right thing, which we'll talk about in a second.",
      "speaker": "Hamel Husain",
      "timestamp": "00:47:56",
      "youtube_link": "https://www.youtube.com/watch?v=BsWxPI9UM4c&t=2876s",
      "context": "Introduces the key mental model that LLM-judge evals require validating the judge itself, not just the system being tested.",
      "insight_type": "framework"
    }
  ]
}