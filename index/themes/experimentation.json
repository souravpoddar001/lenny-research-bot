{
  "id": "experimentation",
  "name": "Experimentation",
  "description": "Experimentation across these episodes is treated as a disciplined operating system for learning under uncertainty\u2014spanning product discovery, growth, monetization, and AI reliability. The theme addresses how to design tests that answer the right questions, choose metrics that reflect real value (not vanity), and build org mechanisms (cadence, ownership, evals) that make iteration fast without becoming random optimization. It matters because most teams either under-test (shipping on opinion) or over-test (local A/B wins that miss strategy, PMF shifts, or long-term effects), and AI adds non-determinism that raises the bar for measurement.",
  "episode_count": 50,
  "episodes": [
    "adam-fishman",
    "adam-grenier",
    "adriel-frederick",
    "aishwarya-naresh-reganti-kiriti-badam",
    "albert-cheng",
    "aparna-chennapragada",
    "archie-abrams",
    "asha-sharma",
    "ben-williams",
    "bill-carr",
    "elena-verna",
    "elena-verna-30",
    "elena-verna-40",
    "eric-ries",
    "gina-gotthilf",
    "grant-lee",
    "hamel-husain-shreya-shankar",
    "hamelshreya",
    "hila-qu",
    "itamar-gilad",
    "jackson-shuttleworth",
    "jake-knapp-john-zeratsky-20",
    "joe-hudson",
    "jonathan-becker",
    "jules-walter",
    "karri-saarinen",
    "kayvon-beykpour",
    "kristen-berman",
    "laura-schaffer",
    "lauryn-isford",
    "marc-benioff",
    "mayur-kamat",
    "melissa-perri",
    "melissa-tan",
    "meltem-kuran",
    "mihika-kapoor",
    "mike-maples-jr",
    "naomi-ionita",
    "nick-turley",
    "noam-lovinsky",
    "ramesh-johari",
    "ray-cao",
    "ronny-kohavi",
    "sanchan-saxena",
    "scott-wu",
    "tanguy-crusson",
    "teresa-torres",
    "tim-holley",
    "timothy-davis",
    "yuriy-timen"
  ],
  "subtopics": [
    "What to test first: diagnosing whether the constraint is PMF, onboarding/activation, retention, pricing, or acquisition before running experiments",
    "Experiment design quality: hypotheses, guardrails, sample sizing/segmentation, and avoiding misleading proxy metrics and short-term wins",
    "Operating cadence and org design: how to structure teams, pipelines, and decision rights to run high-volume, high-quality experiments",
    "Onboarding and activation as an experimentation surface: aligning brand promise to product reality and systematically connecting users to core value",
    "AI-specific experimentation: evals, post-training loops, non-determinism, and how to iterate safely with human judgment + measurement",
    "When experimentation becomes a trap: recognizing \u201crandom acts of A/B testing,\u201d channel-chasing, and optimization that ignores strategy or long-term effects"
  ],
  "key_episodes": [
    "eric-ries",
    "albert-cheng",
    "itamar-gilad",
    "hamel-husain-shreya-shankar",
    "archie-abrams"
  ],
  "common_frameworks": [
    "Build-Measure-Learn (Lean Startup) and MVP thinking (including common misinterpretations and better usage)",
    "Evidence-guided product development: lightweight tests to reduce uncertainty vs plan-and-execute roadmaps",
    "Experimentation operating system: hypothesis backlog, rapid cycles, clear owners, instrumentation, and decision rules",
    "North Star / input vs output metrics and guardrails (balancing leading indicators with durable outcomes)",
    "Working Backwards (customer problem framing + success criteria before building/testing)",
    "AI evaluation loops (evals): curated test sets, human-in-the-loop scoring, regression tracking, and iterative prompt/tool/system changes"
  ]
}